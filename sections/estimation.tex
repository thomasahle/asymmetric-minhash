%! TEX root = ../aminhash.tex

\section{The Estimators}

In this section, we develop a maximum likelihood estimator (MLE) for MinHash, analyse the variance and compare it to the classical MinHash estimator.
% TODO: What can I really say about "lowest possible variance?"
Such an estimator has the lowest possible variance asymptotically as $K\to\infty$, but can be slow to evaluate.
We thus proceed to develop a third estimator that is as fast as the classical estimator while experimentally nearly on par with the MLE in variance.
Using numerical methods we finally show how to get the full power of the MLE in time linear in the number of MinHash values.
% requires nearly as few MinHash values as the MLE for equivalent recall.
%In fact, for $K < 50$ it requires fewer values than even the MLE.

A quick note on notation: If $u\in\mathbb N$, we use $[u]=\{0,\dots,u-1\}$ to indicate the set of numbers up to $u$.
If $P$ is a proposition, we use $[P]$ to indicate the variable that is $1$ if $P$ and $0$ if not $P$.

\subsection{Maximum Likelihood Estimator}\label{sec:mle}

\begin{figure}
   \centering
   \includegraphics[trim=30 5 30 35,clip,width=\linewidth]{figures/scatter}
   \caption{
      Estimated vs. True Jaccard similarity on a random query in the Netflix dataset with 5,000 pairs, using $K=31$.
      The classical estimator can only take $K+1$ different values which leads to some visual banding.
      Our new estimators are freer and overall more concentrated around the diagonal.
   }
   \label{fig:scatter}
\end{figure}

In the classical analysis of MinHash (and consistent hashing)
we imagine two sets $X,Y\subseteq[u]$ being given,
and then compute the probability that a random hash function $h:[u]\to[0,1]$ will pick its smallest value in the intersection $X\cap Y$.
This turns out to depend only on the size of $X\cap Y$, and so it doesn't matter that we don't actually know the sets $X$ and $Y$ while making the estimates.

To improve upon the classical estimator we have to use the fact that we in fact know $h$ at the time of making the estimate.
However, if we just take $h$ as a given in the analysis, there won't be anything random left, and the analysis will end up depending on the values of $Y$.
In case of maximum likelihood estimation the analysis is closely tied to the actual algorithm, so depending on $Y$, which we don't know, is not a good idea.

Our compromise is to assume the values of $h$ on $X$ are known, but not the values of $h$ outside of $X$.
This results in an analysis (and an estimator) that uses the information given to the algorithm about $X$, but doesn't use any information about $Y$, just as in the original analysis.
Note that this model of analysis is only used for deriving the MLE.
When, in \cref{sec:analysis}, we analyse the variance, we will assume $h$ is all unknown and get an expression only in terms of $|X|$, $|Y|$ and $|X\cap Y|$.

We discuss one final model.
Recall that we can equivalently assume $h:[u]\to[u]$.
We call the output of this $h$ the ``rank'', since it describes the placement of the hash value in the order of hash values of $[u]$.
Let $\X=\{h(x) : x\in X\}$ and $\Y=\{h(y):y\in Y\}$.
Then knowing $h$ on $X$ corresponds to knowing $\X$, but sampling $\Y$ uniformly at random.
This is a useful model combinatorially since it completely removes $h$ from the analysis.
\footnote{
   It also suggests a future Bayesian estimator in which $Y$ is not assumed to be uniformly distributed, but follow some prior distribution, such as the ``skewed data'' model of~\cite{mccauley2018set}.
}

\smallskip

Given a random variable and a statistical model, ``estimation'' is the task of inferring the parameters to the model based on the observation.
A maximum likelihood estimator chooses the parameters that maximize the probability that the model generates the particular observed value.
That is, if the observation is equal to $\rho$ with probability $p(\rho;\,\theta)$, $\theta$ is unknown, once we observe $\rho$, we estimate $\theta$ as the $\argmax p(\rho;\,\theta)$.

% A statistical model for a MLE contains four types of variables:
% Those we know at the time of estimation;
% those that are random but we observe;
% those that are random and hidden;
% and those we want to estimate.

In our case we get the following model discussed above:
Given a set $\X$ of size $n_x$ and values $n_y$ and $v$, we sample a set $\Y$ with $|\Y|=n_y$ and $v=|\X\cap \Y|$.
We let $r$ be some MinHash value the estimator may observe.
The log-likelihood of the observation is then:
\[
   \ell(r; v) = \log\Pr_{\Y}[\min\Y = r],
\]
in other words, the probability that a set $\Y$ sampled with overlap $v$ with $\X$ had this particular value as its smallest.

We note that if we do not consider $n_y$ to be known, we can let the model have two parameters (rather than just $v$) and estimate both of them.
This could be done in a Bayesian way by for example counting the frequency of set sizes in the database and build this into the model.
However, in the database model, not much space is saved, since the set size is only one value out of $K$.

%As a first step to computing $\ell(r;v)$ we not that $h$ can be assumed to take values in $[u]$ rather than $[0,1]$ and a be a bijection.
%However then $h$ simply represents a known permutation of $[u]$ and we can ignore it all together.
We define $n_x = |\X|$ and the observed rank $R=\min \Y$.
We also define $m=m(r)=\sum_{x\in \X}[x < r]$ to be the number of values in $\X$ smaller than $r$.
%With this notation we state the following proposition:
\begin{proposition}
   %Let $r\in[u]$ be the MinHash of $Y$ and let $y^*$ (also $\in[i]$) be its index such that $h(y^*)=r$.
\[
   \Pr_Y[R=r]
    =
    \begin{cases}
      \frac{\binom{n_x-m-1}{v-1}\binom{u-r-(n_x-m)}{n_y-v}}{\binom{n_x}{v}\binom{u-n_x}{n_y-v}}
      &
      \text{if $r\in \X$}
       \\
      \frac{\binom{n_x-m}{v}\binom{u-r-1-(n_x-m)}{n_y-v-1}}{\binom{n_x}{v}\binom{u-n_x}{n_y-v}}
      & \text{if $r\not\in \X$}
    \end{cases}
 \]
 \label{prop:prob}
\end{proposition}
%
Note we take $\binom{n}{k}=0$ for $n<k$.
In particular, this may happen if $n_x-m<v$.
The probability of $R=r$ in this case is 0, since $n_x-m$ is the number of x-ranks at least $r$, and all of $\X\cap \Y$ must have rank at least $r$.
\begin{proof}
   Not considering $R$ there are $\binom{n_x}{v}\binom{u-n_x}{n_y-v}$ ways to choose $\Y$ such that $|\Y|=n_y$ and $|\X\cap \Y|=v$.
   We proceed by cases..

   First, consider the case $r\in \X$.
   Then the remaining $v-1$ overlapping elements have to be chosen from $\{x\in \X:x > r\}$.
   by definition of $m$ there are $n_x-m-1$ such values.
   The remaining $n_y-v$ non-overlapping elements have to be chosen from $\{x\not\in X: x > r \}$.
   There are $u-r$ elements in $[u]$ greater than $r$, and of those $n_x-m$ are in $\X$.
   Thus the number of ways to choose $\Y$ with $r\in \X$ is
   $\binom{n_x-m-1}{v-1}\binom{u-r-(n_x-m)}{n_y-v}$.

   The case $r\not\in \X$ follows by similar arguments.
\end{proof}

Using \cref{prop:prob} we can write the log-likelihood in the following concise manner:
\[
   \ell(r; v) = \log \frac{\binom{n_x-m-[r\in \X]}{v-[r\in \X]}\binom{u-r-[r\not\in \X]-(n_x-m)}{n_y-v-[r\not\in \X]}}{\binom{n_x}{v}\binom{u-n_x}{n_y-v}}.
   \label{eq:log-likelihood}
\]
If we observe $K>1$ values $r_1, r_2, \dots, r_K$ we get, by independence of the MinHash functions, a log-likelihood of
\[
   \ell(r_1; v) + \ell(r_2; v) + \dots + \ell(r_K; v).
\]
It is trivial (if not efficient) to enumerate all $v\in[\min\{n_x,n_y\}+1]$ and compute which one has the highest log-likelihood.

We finally define our estimators for intersection ($T_v$) and Jaccard similarity ($T_j$).
\begin{definition}[Maximum Likelihood Estimator (MLE)]
   The maximum likelihood estimators for respectively set overlap and Jaccard similarity are
   \begin{align}
      T_v(r_1,\dots,r_K) &= \argmax_{v\in[\min\{n_x,n_y\}+1]} \ell(r_1; v) + \ell(r_2; v) + \dots + \ell(r_K; v).
      \\T_j(r_1,\dots,r_K) &= \frac{T_v(r_1,\dots,r_K)}{n_x+n_y - T_v(r_1,\dots,r_K)}.
   \end{align}
\end{definition}
%
The re-parametrizing the estimator to Jaccard follows from the two quantities being monotone in each other.
Hence if $T_v$ maximizes the likelihood of $v$, $T_j$ will maximize the likelihood of the Jaccard similarity.

\subsection{Analysis}\label{sec:analysis}

We want to analyse the MLE in the model where $h$ is unknown.
   In this setting, we have for the MinHash estimator $E[T]=\frac1K\sum_i\Pr[q_i(X)=q_i(Y)] = j$ and
\[
   V[T] =
   \frac{E[(T-j)^2]}{K}
      = \frac{E[T^2] - j^2}{K}
      = \frac{\Pr[T=1] - j^2}{K}
      = \frac{j(1-j)}{K}.
      \label{eq:minvar}
\]
The expectation and variance thus depend only on the similarity and not on the specifics of $X$ and $Y$.
%This is important because we want to know the general expected performance of the algorithm, and not what it does after some particular random event.
%TODO: We should talk about bias before variance.

The main work of this section will be proving the following proportion:
\begin{proposition}\label{prop:mle_var}
   As $K\to\infty$, the variance of the MLE converges to
   \[
      \frac{j (1+j)^3 n_y (n_y-j n_x) (n_x-j n_y)}{(n_x+n_y) \left((1+j)^2 n_xn_y - j^2 (n_x+n_y)^2\right)K}
   \label{eq:mle_var}
   \]
   over the randomness of the random hash function.
\end{proposition}

\begin{figure}
\includegraphics[trim=30 0 0 0,clip,width=\linewidth]{figures/mle_variance2}
\caption{Variance of maximum likelihood estimator based on Fischer Information bound.
For $j$ close to 0 or 1 the worst case MLE bound is asymptotically equal to the Classical bound, whereas for $j\approx 0.21$ has only $\approx 62\%$ of the variance.
See \cref{fig:exp_variance} for a corresponding experimental test.
}
\label{fig:mle_variance}
\end{figure}

The bound is a little complicated by the fact that it includes the sizes of the sets $n_x$ and $n_y$.
We note that the bound is convex in the ratio $n_y/n_x$, giving lower variances than \cref{eq:minvar} for $n_x<\!<n_y$ or $n_x >\!> n_y$.
\Cref{fig:mle_variance} shows \cref{eq:mle_var} when the ratio is taken to be \emph{worst possible} as a function of $j$, as well as when it is taken to be 1.
In the symmetric case $n_x=n_y$ the asymptotic variance reduces to
\[
   \frac{j(1-j)}{K}\frac{(1+j)^3}{2(1+3j)},
\]
which is easy to compare with \cref{eq:minvar} since $\frac{(1+j)^3}{2(1+3j)} \in [\frac12,1]$ for all $j\in[0,1]$.
%for large $x$ and $y$.
For $n_x/n_y\neq1$ the Jaccard similarity is bounded above by $\frac{\min\{n_x,n_y\}}{\max\{n_x,n_y\}}$ which the MLE exploits and discards those higher values from consideration, resulting in 0 variance in that range.
%Another interesting point from \cref{eq:mle_var} is that the variance at $n_x=c n_y$ is exactly $1/c$ of the variance at $n_x=n_y/c$.
For small $j$ \cref{eq:mle_var} is $\frac{n_y j}{n_x+n_y}-O(j^2)$ compared with $j-O(j^2)$ for \cref{eq:minvar}.
It makes sense that the variance is lower when $|X|$ is big compared to $|Y|$, since we are given $X$, but don't know $Y$.

The global risk of an estimator is defined as the worst possible variance over the parameter space.
In the case of the classical MinHash estimator, the global risk is $1/(4K)$ at $j=1/2$.
We can also compute the global risk of the MLE, which is $0.1788/K$, $28.5\%$ less than the classical estimator.

\smallskip

Recall the model of the analysis is the same as for classical MinHash:
Given $X$ and $Y$ we sample a random hash function $h:[u]\to[u]$.
We compute $r=\min_{y\in Y}h(y)$ and $m=\sum_{x\in X}[h(x)<r]$.

\begin{proof}[Proof of \cref{prop:mle_var}]
   We first find the variance for the MLE for $v$ and then show how to re-parametrize it to use the Jaccard similarity.

   Using Stirling's approximation $ \log n! = n\log n - n + O(\log n)$,
   we rewrite the log-likelihood \cref{eq:log-likelihood} as
   \begin{align}
      \ell(r;v) &=
      (n_x-m-[r\in\X]+1)H(\tfrac{v-[r\in\X]}{n_x-m-[r\in\X]+1})
               - (n_x+1) H(\tfrac{v}{n_x+1})
              \\&+(u-r-[r\not\in\X]-n_x+m+1) H(\tfrac{n_y-v-[r\not\in\X]}{u-r-[r\not\in\X]-n_x+m+1})
              \\& -(u-n_x+1) H(\tfrac{n_y-v}{u-n_x+1})
   + O(\log u),
   % TODO: We should probably also have error terms for n_y-v, n_x-v and so on.
   \end{align}
   where $H(p)=p \log \frac{1}{p} + (1-p)\log \frac{1}{1-p}$ is entropy function.

   Standard results~\cite{panchenko2016lec3} on maximum likelihood estimators say that
   the variance converges to $1/I(v)$ where
   \[
      I(v) = E\left[-\frac{d^2}{dv^2}\ell(r; v)\right]
   \]
   is known as the Fischer information.
   \footnote{This is a bit more tricky than it seems, since
      the standard proof of this fact~\cite{panchenko2016lec3} uses that the expectation is taken over the same probability space as $\ell$ is defined.
      However, one can check that the key step in which that is used is to show
      $E[f''/f] = \int (f''(x)/f(x))f(x)dx = \int f''(dx) = (\int f(x)dx)'' = 0$, where $f=\exp(\ell)$ is the probability distribution.
      Since $E_h[f''/f] = E_h[E_{h_{|\overline X}}f''/f] = E_h[0]= 0$
      we can run the entire proof using $E_h$ rather than $E_{h_{|\overline X}}$ and get the same result.
      Thus it suffices to thus focus on bounding $E_h[-\frac{d^2}{dv^2}\ell(r;v)]$.
   % Something, something sigma algerba.
      % Basically I'm arguing that we can run the whole proof of MLE asymptotics and consider very expectatation as over $E_h$ rather than $E_{h_{|X}}$
   }

   % Strictly speaking the Fischer information is only defined when the log-likelihood is differentiable in the parameter $j$, which is not the case here, since we know $\frac{j}{1+j}(x+y)$ is an integer.
   %Since Stirling's approximation also applies to derivatives (simply exchanging the sum and the derivative operator)


   We can now evaluate the first two derivatives:
   {
      \thinmuskip=0mu
      \begin{align}
         % Single diff
         \frac{d}{dv}\ell(r;v)
         &=
         \log\left(\frac{(1-\frac1{n_y-v+1})^{[r\not\in\X]}(1-\frac{m}{n_x-v+1})}{(1-\frac 1{v+1})^{[r\in\X]}(1-\frac{r-m}{u-n_x-n_y+v+1})}\right) + O(1/u)
         \label{eq:deriv1}
         \\
         % Double diff
         \frac{d^2}{dv^2}\ell(r;v)
        &=
          [r\not\in\X](\tfrac1{n_y-v+1}-\tfrac1{n_y-v})
         + (\tfrac1{n_x-v+1}-\tfrac1{n_x-v-m+1})
      \\&+[r\in\X](\tfrac1{v+1}-\tfrac1{v})
         + (\tfrac1{u-n_x-n_y+v+1}-\tfrac1{u-n_x-n_y+v-r+m+1})
      \\&+ O(1/u^2)
         \label{eq:deriv2}
         .
      \end{align}
      \vspace{-1em}
   }

   We now have three terms to bound:
   $E[r\in\X]$, $\tfrac1{n_x-v-m+1}$ and $\tfrac1{u-n_x-n_y+v-r+m+1}$.
   Since any element of $Y$ has even chance of becoming the smallest under $h$, we have
   \[E[r\in\X] = \Pr[r\in\X] = \frac{v}{n_y}. \]

   When considering the distribution of $r$ and $m$, we will assume the values of $h$ have exponential distribution, $Exp(1)$, instead of uniform over $[0,1]$.
   This corresponds to using $\log1/h(x)$ instead of $h(x)$ which is a strictly monotone transformation and so equivalent in terms of comparing hash values.
   Let $y^* = \argmin_{y\in Y}h(y)$ and $h^* = h(y^*)$.
   (Note this is different from $r$, which is the rank.)
   We then have that Then $h^* \sim \text{Exp}(y)$ by the stability of minimums of exponentially distributed random variables.

   We can now see $m=\sum_{x\in X\setminus Y} [h(x) < h^*]$ as having binomial distribution $B(n_x-v, p)$, conditioning on $h^*$, where $p=\Pr[h(x)\le h^*] = 1-\exp(-h^*)$ by the CDF for the exponential distribution.
   (We only sum over $X\setminus Y$ rather than all of $X$ since no value in $X\cap Y$ can be smaller than $h^*$ by definition.)
   Because of the binomial revision
   $\frac1{n-i+1}\binom{n}{i} = \frac1{n+1}\binom{n+1}{i}$
   we can evaluate
   \begin{align}
      E_m\left[\tfrac1{n_x-v-m+1}\right]
      &= E_{h^*}\left[\tfrac{1-p^{n_x-v+1}}{(1-p)(n_x-v+1)}\right]
    \\&= \tfrac1{n_y-1}\left(\tfrac{n_y}{n_x-v+1} - \tfrac1{\binom{n_x+n_y-v}{n_y}}\right),
    % TODO: Does something weird happen if v=n_x or v=n_y?
   \end{align}
   where the second equality follows by an integral over the Beta function.
   Note that the expectation is defined for all $n_y\ge 0$ by limits.%
   \footnote{In particular at $y=1$ it equals $H_{n_x-v+1}/(n_x-v+1)$, where $H_n$ is the harmonic number.}

   We can similarly note that $r-m$ is the number of values in the complement of $X\cup Y$, and so has binomial distribution $B(u-n_x-n_y+v, p)$.
   By the same arguments as above, we get that
   \[
      E\left[\tfrac1{u-n_x-n_y-v-(r-m)+1}\right]
      = \tfrac1{n_y-1}\left(\tfrac{n_y}{u-n_x-n_y+v+1} - \tfrac1{\binom{u-n_x-n_y+v}{y}}\right).
   \]
   Combining all the terms of \cref{eq:deriv2}, and assuming $n_x$ and $n_y$ sufficiently large we get the simple result
   \[
   I(v)
   = \frac{1}{n_y(n_x-v)} + \frac1{v(n_y-v)} + O\left( \frac{1}{\min\{n_x,n_y\}} \right).
   \]
   We can now use the re-parametrization formula for Fischer Information to compute
   \[
      %I_j(j) = j'(v(j))^{-2}I_v(v(j))
      I_j(j) = v'(j)^{2}I_v(v(j)),
   \]
   %where $j(v) = v/(x-y+v)$.
   where $v(j) = \frac{j}{1+j}(x+y)$.
   % eta = psi(theta)
   % j = j(v)
   % theta = psi^{-1}(eta)
   % v = j^{-1}(j)
   % I(j) = I(v)
   By the previously stated facts on maximum likelihood estimators, this proves the proposition.
\end{proof}

We have succeeded in analysing the variance of the maximum likelihood estimator.
There are more questions to ask, such as how large $K$ must be to start seeing convergence to the stated bound.
We give some experimental evidence for these questions in \cref{sec:estimation}.

\subsection{Minner Estimator}\label{sec:minner}

\begin{figure}
   \includegraphics[trim=10 0 45 40,clip,width=\linewidth]{figures/synvar_100000.png}
   \caption{Measured variance of estimators, over 100,000 repetitions at $|X|=|Y|=K=30$ and $u=500$.
      The MLE has already almost converged to \cref{fig:mle_variance}.
      The Minner Estimator is seen to be particularly good for low Jaccard similarities, which may be why it works so well on the practical datasets tested in \cref{sec:evaluation} which tend to have similarities concentrated in the $[0,0.2]$ range, as seen in \cref{fig:scatter}.}
   \label{fig:exp_variance}
\end{figure}

In the previous sections, we derived and analysed a Jaccard similarity estimator based on maximum likelihood.
The estimator has to evaluate the log-likelihood function, $\ell$, for all $v\in[\min\{n_x,n_y\}+1]$, which means it takes time at least $\Omega(K\min\{n_x,n_y\})$ per database point.
In this section we investigate numerical methods for speeding up the MLE and suggests a new, fast estimator which can be computed as fast as the classical MinHash estimator.

We call this the ``Minner Estimator'' since it is based on counting the number of elements in $X$ that hash to a value smaller than the minimum hash value of $Y$.
The expected number of such values is $(|X\setminus Y|)/(|Y|+1)$ since each element of $X\setminus Y$ has a probability $1/(|Y|+1)$ of being smaller than all in $Y$, and the values in $X\cap Y$ can't by definition be.
If $M$ is the Minner count, then $|X| - M(|Y|+1)$ is an unbiased estimator for the intersection size $v=|X\cap Y|$.
However, we will derive a much better estimator based on considerations about the MLE from the previous section.

The starting point of this derivation is the continuous derivative log-likelihood \cref{eq:deriv1},
which we would like to solve $=0$ for $v$.
If we apply the approximation $\log(1-\eps) \approx -\eps$,
we get
\[
   \frac{d}{dv}\ell(r;v) \approx
   -\frac{[y^*\not\in X]}{n_y-v} 
   -\frac{m}{n_x-v} 
   +\frac{[y^*\in X]}{v} 
   +\frac{r-m}{u-n_x-n_y+v} 
   %= 0
   .
\]
This is a convenient form since it is linear in the variables, $[y^*\in X]$, $m$ and $r$.
As we observe multiply $r_i$ values, we can define
$R = \sum_i r_i$, $M = \sum_i m_i$ and $C = \sum_i [y_i^*\in X]$.
This gives us a single equation to solve
\[
   \sum_i\frac{d}{dv}\ell(r_i; v) \approx
   -\frac{K-C}{n_y-v} 
   -\frac{M}{n_x-v} 
   +\frac{C}{v} 
   +\frac{R-M}{u-n_x-n_y+v} 
   = 0
   .
   \label{eq:d1_simple}
\]
This equation can be rewritten as a degree three polynomial and solved by standard methods.
The time complexity has thus been decreased from $\Omega(K\min\{n_x,n_y\})$ to $O(K)$ plus the time it takes to find the polynomial roots.

However, solving a polynomial for every point in the database is hardly as fast as the classical MinHash estimator.

However, we would like a simpler estimator still.
In set data, $u$ is normally very large, so we will approximate $\frac{R-M}{u-n_x-n_y+v}\approx 0$.
If we assume $n_y>\!>v$ we may approximate $\frac{K-C}{n_y-v}\approx 0$ we get the simple solution to \cref{eq:d1_simple}, $v=\frac{C n_x}{C+M}$.
Alternatively, if $n_x>\!>v$ we approximate $\frac{M}{n_x-v}\approx 0$, we get $v=\frac{C n_y}{K}$.
We then combine the two approximations into the following estimator:
\begin{definition}[Minner Estimator]
\[
   T_v(r) = \min\left\{\frac{C n_x}{C+M}, \frac{C n_y}{K}\right\}.
   \label{eq:minner}
\]
\end{definition}

The resulting value is clearly always in the acceptable range $[0,\min\{n_x, n_y\}]$ since $C\le C+M$ and $C \le K$.
To estimate Jaccard we take $T_j(r) = v/(n_x + n_y - v)$.
As before we can compute $C$ and $M$ in $O(K)$ time per database point, and now we have replaced the finalization of finding the roots of a polynomial with a simple division.

\smallskip

While $E[\frac{C n_y}{K}] = v$ is nice and unbiased (for estimating $v$), the combined estimator is not necessarily so.
Using the arguments from the previous analysis section, we find that for a single observation,\footnote{We were not able to analyse the case where $C$ and $M$ are the sum of multiple $c_i$ and $m_i$ values, nor the effect of combining the two estimators using the minimum.}
\begin{align}
   E[\frac{c n_x}{c+m}]
   &= \frac{v n_x}{y} E[\frac{1}{1+m}]
   = \frac{v n_x}{n_x-v+1} (H_{n_x+n_y-v} - H_{n_y-1})
 \\&\approx \frac{v n_x}{n_x-v+1} \log\frac{n_x+n_y-v}{n_y-1},
 \label{eq:minner_mean}
\end{align}
where $H_n$ is the $n$th Harmonic Number.
If we let $n_x=n_y=n\to \infty$ we get
\[\eqref{eq:minner_mean} = \frac{2j}{1-j}\log\frac{2}{1+j},\]
a quantity sandwiched between the Jaccard similarity, $j$, and the Sørensen–Dice coefficient, $\frac{2j}{1+j}$.
While not unbiased, this is at least monotone in $j$ (respectively $v$).

Experimentally, the actual Minner estimator seems to converge to $j$ for larger $K$ and $j$ not too large.
That is, the estimator which uses the sums $C$ and $M$, rather than taking the mean of $\frac{c_i n_x}{c_i+m_i}$, and which makes the minimum with $Cn_y/K$.
%See~\cref{fig:var}.
For larger Jaccard similarities Minner seems to slightly underestimate Jaccard, just as we see on the variance get worse as $j\to1$ in~\cref{fig:exp_variance}.

\smallskip

We finally present a numerical way to combine the speed of the Minner estimator with the consistency of the MLE.
The idea is a common one in MLE design, which is to apply Newton's method to the problem of solving $\frac{d}{dv}\ell(r;v)=0$.%
\footnote{People sometimes use Newton's method with the expected Hessian instead of the actual second derivative~\cite{longford1987fast}, however, in our case, we'll be able to efficiently compute it exactly.}
To maintain $O(K)$ complexity per database point we apply Newton's method to the approximate derivative equation \cref{eq:d1_simple}, which provides the second derivative is still linear in $C$, $R$ and $M$:
\[
   \sum_i\frac{d^2}{dv^2}\ell(r_i; v) \approx
   \frac{K-C}{(n_y-v)^2} 
   +\frac{M}{(n_x-v)^2} 
   +\frac{C}{v^2}
   +\frac{R-M}{(u-n_x-n_y+v)^2}
   .
   \label{eq:d2_simple}
\]
Newton's method now proceeds with iterations
$v_{i+1} = v_i - \frac{\ell'(r; v_i)}{\ell''(r; v_i)}$.

This concludes the derivation of the Minner estimator with Newton refinement.
In the next section, we give pseudo-code matching what was used to perform our experiments on real-world data.

